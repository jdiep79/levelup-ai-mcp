**ğŸ“˜ Intro**

In this video, Ethan demonstrates a complete hands-on integration of a real-time documentation-fetching **MCP server** using the `lm.txt` standard. This server is designed to dynamically retrieve **up-to-date documentation** from official sources like LangGraph, solving the problem of stale knowledge in AI-generated responses. Using **Cloud Desktop**, Ethan shows how the server plugs in via MCP, debugs setup issues, and ultimately proves its value by grounding LLM answers with fresh web content.

[MCP LLMS-TXT Documentation Server](https://github.com/langchain-ai/mcpdoc)

![alt text](image-2.png)

---

**ğŸ“š Summary**

- The showcased **MCP server scrapes live documentation** based on a siteâ€™s `lm.txt`.
- The LLM (via Cloud Desktop) uses tools exposed by the server to:

  1. Discover content structure from `lm.txt`.
  2. Identify relevant URLs.
  3. Fetch real-time content for answering.

- This workflow guarantees **non-stale**, **live data** for agent responses.
- Tools like **MCP Inspector** and careful terminal debugging help ensure the server is correctly configured and connected.

---

**ğŸ—ï¸ Key Components and Workflow**

### ğŸ§  Server Purpose

- Avoid indexing documentation statically (which quickly becomes outdated).
- Instead, **scrape live content** from official docs using the `lm.txt` index.
- Example target: LangGraphâ€™s evolving documentation.

### ğŸ§© How It Works (Conceptually)

1. ğŸ“– Load `lm.txt` = like a table of contents.
2. â“ Parse the user's query (e.g., "What is LangGraph memory?").
3. ğŸ” Select the relevant documentation section (via URL in `lm.txt`).
4. ğŸŒ Fetch and scrape that page content.
5. ğŸ“¤ Provide a grounded, real-time answer.

---

**ğŸ”§ Setup & Execution**

### ğŸ“¥ Clone & Prepare the MCP Server

```bash
git clone https://github.com/anthropic-ai/mcp-doc
cd mcp-doc
uv venv  # Create virtual environment
source .venv/bin/activate
uv pip install -r requirements.txt
```

### ğŸš€ Run the Server (SSE or stdio)

#### Example: Run with SSE and LangGraph docs

```bash
uv run main.py --doc-url=https://docs.langgraph.dev/py/lm.txt
```

- Runs on port `8082` (SSE mode)
- Later tested with **Cloud Desktop** in `stdio` mode on port `8081`

---

**ğŸ§ª Testing with MCP Inspector**

1. Start Inspector via:

   ```bash
   npx mcp-inspector
   ```

2. Connect to `localhost:8082`
3. View exposed tools:

   - `list_doc_sources`: Returns the `lm.txt` URL
   - `fetch_docs`: Scrapes and returns content from a given URL

4. Use input fields to test tool output

---

**âš™ï¸ Cloud Desktop Integration**

1. ğŸ§‘â€ğŸ’» Go to:

   ```
   Settings â†’ Developer â†’ MCP Config
   ```

2. ğŸ“ Paste the config snippet:

   ```json
   {
     "name": "lm-docs",
     "command": "/absolute/path/to/uv",
     "args": [
       "run",
       "main.py",
       "--doc-url=https://docs.langgraph.dev/py/lm.txt"
     ],
     "transport": "stdio",
     "port": 8081
   }
   ```

3. ğŸ§ª Restart Cloud Desktop

4. âœ… Use the MCP Tool:

   - Ask: "What is LangGraph memory?"
   - Agent:

     - Runs `list_doc_sources`
     - Gets URL to `lm.txt`
     - Runs `fetch_docs` on the `.txt` to find structure
     - Selects the **memory** URL
     - Runs `fetch_docs` again to retrieve memory content
     - Responds with real-time, grounded documentation

---

**ğŸ Debugging Tips**

- â— Errors running `uv`?

  - Use `which uv` to find the full path
  - Use absolute paths in `command`

- ğŸ§ª Always restart Cloud Desktop after MCP config changes
- âœ… Use Inspector to verify server and tool health before integration

---

**ğŸŒŸ Key Benefits**

| Feature                              | Impact                                                             |
| ------------------------------------ | ------------------------------------------------------------------ |
| ğŸ•’ Real-time scraping                | Keeps docs fresh, avoids stale answers                             |
| ğŸ§° Modular MCP tools                 | Easily plugged into multiple apps (Cursor, Cloud, Windsurf)        |
| ğŸ” Structured discovery via `lm.txt` | Helps agents reason about site layout                              |
| ğŸ§  LLM reasoning + scraping tools    | Auto-fetch only whatâ€™s relevant                                    |
| ğŸ“ `lm.txt` = Index                  | `lm-full.txt` = Full data dump (for offline use, vector embedding) |

---

**ğŸ†š Comparison: Trained Model vs. Live Integration**

| Without MCP               | With MCP                         |
| ------------------------- | -------------------------------- |
| Answer from training data | Answer from live scraped content |
| May be outdated           | Guaranteed fresh                 |
| No explainability         | Shows tool chain + source        |
| Passive                   | Interactive, agentic             |

---

**ğŸ” Security Note**

- Server fetches public content, but always ensure scraping obeys `robots.txt`.
- MCP servers could be extended to support authentication for gated APIs.

---

**ğŸ”œ Coming Up**

> Ethan will:
>
> - Dive deeper into prompt use with `lm.txt`
> - Explore real-time vs. cached retrieval trade-offs
> - Possibly demo Docker deployments or remote integrations

This was a powerful demo of how MCP + `lm.txt` can create **live, smart, and grounded agents**. ğŸš€ğŸ“–ğŸ¤–
