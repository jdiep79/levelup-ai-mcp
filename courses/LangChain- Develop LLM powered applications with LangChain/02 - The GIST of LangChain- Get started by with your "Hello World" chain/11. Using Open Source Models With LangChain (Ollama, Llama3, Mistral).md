## 🧠 Running Open Source LLMs Locally with LangChain & Ollama

---

### 📦 **Intro**

In this session, Eden walks you through using **open-source language models like LLaMA 3 and Mistral** in your **LangChain application**, running **locally on your machine** via **Ollama**. While first-tier models like GPT-4 still shine for complex tasks, local models provide speed, privacy, and flexibility for many use cases. LangChain makes swapping models incredibly easy!

---

### 📝 **Summary**

You’ll learn how to:

- ✅ Install & run **Ollama**
- ✅ Download open-source models like **LLaMA 3** and **Mistral**
- ✅ Switch between **OpenAI and local models** in LangChain
- ✅ Parse LLM output with **StringOutputParser**
- ✅ Evaluate quality tradeoffs between model tiers
- ✅ Avoid pitfalls of reasoning tasks with lightweight models

---

### 🛠️ **Setup Steps**

#### 1️⃣ Install Ollama

Go to [https://ollama.com](https://ollama.com) → Download for your OS
💻 Eden used macOS, but Windows/Linux is supported.

#### 2️⃣ Run & Verify Ollama

```bash
ollama run llama3
```

- Downloads and runs **LLaMA 3** model.
- You can chat via terminal as a quick sanity check.

Try:

```bash
ollama run llama3
> What model am I talking to?
```

---

### 🧩 **Integrate with LangChain**

#### 3️⃣ Install LangChain Ollama Package

```bash
pipenv install langchain-ollama
```

#### 4️⃣ Swap from OpenAI to Local Model

Replace:

```python
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
```

With:

```python
from langchain_ollama import ChatOllama
llm = ChatOllama(model="llama3", temperature=0)
```

Now your app uses LLaMA 3 running on your local machine!

---

### 🧠 Add Output Parsing

To clean up your result and extract only the text:

```python
from langchain.output_parsers import StrOutputParser
chain = prompt | llm | StrOutputParser()
```

✅ This extracts the `.content` field from the LLM response.

---

### 🔁 Try Another Model: Mistral

#### 5️⃣ Download and Run Mistral

```bash
ollama pull mistral
```

Switch your model in code:

```python
llm = ChatOllama(model="mistral", temperature=0)
```

🧪 Testing:

- ❌ Mistral may fail to summarize Elon Musk properly.
- ✅ It can successfully generate a **song** when prompted.

This illustrates the **importance of task-model fit**.

---

### 🔍 Key Considerations: Open-Source vs. Tier 1

| Feature     | Open Source (LLaMA, Mistral)  | Tier 1 (GPT-4, Gemini) | Emoji |
| ----------- | ----------------------------- | ---------------------- | ----- |
| Latency     | Faster (if local GPU is good) | Cloud-dependent        | ⚡    |
| Cost        | Free (after setup)            | Pay-per-use 💰         | 💸    |
| Privacy     | Local data, 100% private      | Data sent to vendor    | 🔐    |
| Reasoning   | Often weaker for agents 🧠    | Strong performance     | 🧠✅  |
| Setup       | More complex                  | Plug-and-play          | ⚙️    |
| Maintenance | On you                        | On the vendor          | 🔧    |

---

### 🔑 Key Concepts Recap

| Concept              | Description                             | Emoji |
| -------------------- | --------------------------------------- | ----- |
| **Ollama**           | Tool to run models locally              | 💻    |
| **ChatOllama**       | LangChain wrapper for Ollama models     | 🧩    |
| **Model Swapping**   | Just change `ChatOpenAI` ➝ `ChatOllama` | 🔁    |
| **OutputParser**     | Extracts clean response from AI message | 🧾    |
| **Task Sensitivity** | Models vary in strengths per use case   | 🧪    |

---

### ⚠️ Deployment & Production Advice

Eden’s tips:

- First-tier models are **preferred** for agentic & complex reasoning apps 🔍
- Open-source models are great for:

  - Summarization 📝
  - Entity extraction 🧠
  - Content generation 🎵

- Production-ready use of local models requires:

  - Availability
  - Scalability
  - Monitoring
  - Cost tradeoff consideration

---

### ✅ Final Takeaway

You now know how to:

- Run LLaMA or Mistral **locally with Ollama**
- Use them **interchangeably with LangChain**
- Decide **which model fits which task**
- Keep your workflows private and efficient 🧠🔐

> Open-source LLMs are democratizing GenAI.
> Use them wisely, and always **benchmark** before choosing one! 🧪🚀
