## ğŸ§  Running Open Source LLMs Locally with LangChain & Ollama

---

### ğŸ“¦ **Intro**

In this session, Eden walks you through using **open-source language models like LLaMA 3 and Mistral** in your **LangChain application**, running **locally on your machine** via **Ollama**. While first-tier models like GPT-4 still shine for complex tasks, local models provide speed, privacy, and flexibility for many use cases. LangChain makes swapping models incredibly easy!

---

### ğŸ“ **Summary**

Youâ€™ll learn how to:

- âœ… Install & run **Ollama**
- âœ… Download open-source models like **LLaMA 3** and **Mistral**
- âœ… Switch between **OpenAI and local models** in LangChain
- âœ… Parse LLM output with **StringOutputParser**
- âœ… Evaluate quality tradeoffs between model tiers
- âœ… Avoid pitfalls of reasoning tasks with lightweight models

---

### ğŸ› ï¸ **Setup Steps**

#### 1ï¸âƒ£ Install Ollama

Go to [https://ollama.com](https://ollama.com) â†’ Download for your OS
ğŸ’» Eden used macOS, but Windows/Linux is supported.

#### 2ï¸âƒ£ Run & Verify Ollama

```bash
ollama run llama3
```

- Downloads and runs **LLaMA 3** model.
- You can chat via terminal as a quick sanity check.

Try:

```bash
ollama run llama3
> What model am I talking to?
```

---

### ğŸ§© **Integrate with LangChain**

#### 3ï¸âƒ£ Install LangChain Ollama Package

```bash
pipenv install langchain-ollama
```

#### 4ï¸âƒ£ Swap from OpenAI to Local Model

Replace:

```python
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
```

With:

```python
from langchain_ollama import ChatOllama
llm = ChatOllama(model="llama3", temperature=0)
```

Now your app uses LLaMA 3 running on your local machine!

---

### ğŸ§  Add Output Parsing

To clean up your result and extract only the text:

```python
from langchain.output_parsers import StrOutputParser
chain = prompt | llm | StrOutputParser()
```

âœ… This extracts the `.content` field from the LLM response.

---

### ğŸ” Try Another Model: Mistral

#### 5ï¸âƒ£ Download and Run Mistral

```bash
ollama pull mistral
```

Switch your model in code:

```python
llm = ChatOllama(model="mistral", temperature=0)
```

ğŸ§ª Testing:

- âŒ Mistral may fail to summarize Elon Musk properly.
- âœ… It can successfully generate a **song** when prompted.

This illustrates the **importance of task-model fit**.

---

### ğŸ” Key Considerations: Open-Source vs. Tier 1

| Feature     | Open Source (LLaMA, Mistral)  | Tier 1 (GPT-4, Gemini) | Emoji |
| ----------- | ----------------------------- | ---------------------- | ----- |
| Latency     | Faster (if local GPU is good) | Cloud-dependent        | âš¡    |
| Cost        | Free (after setup)            | Pay-per-use ğŸ’°         | ğŸ’¸    |
| Privacy     | Local data, 100% private      | Data sent to vendor    | ğŸ”    |
| Reasoning   | Often weaker for agents ğŸ§     | Strong performance     | ğŸ§ âœ…  |
| Setup       | More complex                  | Plug-and-play          | âš™ï¸    |
| Maintenance | On you                        | On the vendor          | ğŸ”§    |

---

### ğŸ”‘ Key Concepts Recap

| Concept              | Description                             | Emoji |
| -------------------- | --------------------------------------- | ----- |
| **Ollama**           | Tool to run models locally              | ğŸ’»    |
| **ChatOllama**       | LangChain wrapper for Ollama models     | ğŸ§©    |
| **Model Swapping**   | Just change `ChatOpenAI` â `ChatOllama` | ğŸ”    |
| **OutputParser**     | Extracts clean response from AI message | ğŸ§¾    |
| **Task Sensitivity** | Models vary in strengths per use case   | ğŸ§ª    |

---

### âš ï¸ Deployment & Production Advice

Edenâ€™s tips:

- First-tier models are **preferred** for agentic & complex reasoning apps ğŸ”
- Open-source models are great for:

  - Summarization ğŸ“
  - Entity extraction ğŸ§ 
  - Content generation ğŸµ

- Production-ready use of local models requires:

  - Availability
  - Scalability
  - Monitoring
  - Cost tradeoff consideration

---

### âœ… Final Takeaway

You now know how to:

- Run LLaMA or Mistral **locally with Ollama**
- Use them **interchangeably with LangChain**
- Decide **which model fits which task**
- Keep your workflows private and efficient ğŸ§ ğŸ”

> Open-source LLMs are democratizing GenAI.
> Use them wisely, and always **benchmark** before choosing one! ğŸ§ªğŸš€
